{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc4d53f-2b13-43a1-a554-82b06b0cf764",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaedfdf-fc0d-4ff0-b21d-a9097fb241fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"global\": {\n",
      "    \"kafka_bootstrap_servers\": \"kafka:9092\",\n",
      "    \"kafka_topic\": \"test-structured-streaming\",\n",
      "    \"kafka_consumer_group\": \"ss_job\",\n",
      "    \"max_records_per_batch\": 20\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"consumer\")\n",
    "\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92965dc3-a7c7-40ea-bfdb-1e02340b3713",
   "metadata": {},
   "source": [
    "# 2. Initialize Spark with Kafak Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed753070-30f8-4883-97aa-4147a6532147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.google.guava#guava added as a dependency\n",
      "org.apache.httpcomponents#httpcore added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f58a2436-6088-473b-9285-b1662c3c5ea0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.google.guava#guava;21.0 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.8 in central\n",
      ":: resolution report :: resolve 517ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;21.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.8 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 by [org.apache.kafka#kafka-clients;2.8.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   1   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f58a2436-6088-473b-9285-b1662c3c5ea0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/15 06:33:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:consumer:Spark Driver memory: None\n",
      "INFO:consumer:Spark Executor memory: None\n",
      "INFO:consumer:Loaded jars:\n",
      "[\n",
      "  \"spark://41f56943b782:33451/jars/com.google.code.findbugs_jsr305-3.0.0.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/com.google.guava_guava-21.0.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.apache.httpcomponents_httpcore-4.4.8.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.lz4_lz4-java-1.8.0.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.apache.commons_commons-pool2-2.11.1.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.apache.kafka_kafka-clients-2.8.1.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.apache.hadoop_hadoop-aws-3.3.0.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.spark-project.spark_unused-1.0.0.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/commons-logging_commons-logging-1.1.3.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.slf4j_slf4j-api-1.7.32.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar\",\n",
      "  \"spark://41f56943b782:33451/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Configuration and set application name\n",
    "conf = SparkConf().setAppName(\"KafkaExp\")\n",
    "\n",
    "# Default pyspark installation lacks kafka consumer libraries. Install kafka-client libs manually\n",
    "kafka_packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{\"2.12\"}:{\"3.3.0\"}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0',\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.0\",\n",
    "    \"com.google.guava:guava:21.0\",\n",
    "    \"org.apache.httpcomponents:httpcore:4.4.8\"\n",
    "]\n",
    "\n",
    "# Provide kafka jar paths to driver and executors\n",
    "kafka_jar_paths = '/mnt/home/prathyush/.ivy2/jars/'.join([\n",
    "    \"org.apache.hadoop_hadoop-client-runtime-3.3.2.jar\",\n",
    "    \"org.apache.kafka_kafka-clients-2.8.1.jar\",\n",
    "    \"hadoop-aws-2.7.5.jar\",\n",
    "    \"aws-java-sdk-core-1.12.268.jar\"\n",
    "])\n",
    "\n",
    "# Connect to Spark cluster (Cluster mode instead of local mode)\n",
    "conf = (conf.setMaster('spark://spark:7077')\n",
    "        .set('spark.jars.packages', ','.join(kafka_packages))\n",
    "        .set('spark.driver.extraClassPath', '/mnt/home/prathyush/.ivy2/jars/*')\n",
    "        .set('spark.executor.extraClassPath', '/mnt/home/prathyush/.ivy2/jars/*')\n",
    "        )\n",
    "\n",
    "# Create spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "logger.info(f\"Spark Driver memory: {sc._conf.get('spark.driver.memory')}\")\n",
    "logger.info(f\"Spark Executor memory: {sc._conf.get('spark.executor.memory')}\")\n",
    "logger.info(\n",
    "    f'Loaded jars:\\n{json.dumps((sc._jsc.sc().listJars().toList().toString().replace(\"List(\", \"\").replace(\")\", \"\").split(\", \")), indent=2)}')\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "spark.conf.set(\"parquet.enable.summary-metadata\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b4715-66b6-4a47-8511-fd57bf5a0b52",
   "metadata": {},
   "source": [
    "# 3. Test Kafka topic and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd64ee87-38ef-48fc-9c3a-cb02eea054b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka Connection successful!\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "def test_kafka_connection(broker_conf:dict) -> None:\n",
    "    \"\"\"\n",
    "    Function to test kafka connection\n",
    "    :param broker_conf: Broker configuration\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    client = AdminClient(broker_conf)\n",
    "    topics = client.list_topics().topics\n",
    "    if not topics:\n",
    "        raise RuntimeError()\n",
    "    print(\"Kafka Connection successful!\")\n",
    "\n",
    "\n",
    "broker_conf = {\n",
    "    'bootstrap.servers': config[\"global\"][\"kafka_bootstrap_servers\"]\n",
    "}\n",
    "\n",
    "# Test kafka connection\n",
    "test_kafka_connection(broker_conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56812b03-56c0-45d2-8e3f-8e96fb354d86",
   "metadata": {},
   "source": [
    "# 4. Load Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7ef60c-be21-4f7c-be0d-d02115d52d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('awards', ArrayType(StructType([StructField('award', StringType(), True), StructField('by', StringType(), True), StructField('year', StringType(), True)]), True), True), StructField('birth', StringType(), True), StructField('contribs', ArrayType(StringType(), True), True), StructField('death', StringType(), True), StructField('name', StructType([StructField('aka', StringType(), True), StructField('first', StringType(), True), StructField('last', StringType(), True)]), True), StructField('title', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Load schema \n",
    "schema = pickle.load(open(\"schema.pkl\", 'rb'))\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3505e-9cef-4148-a0af-10643d9d94c7",
   "metadata": {},
   "source": [
    "# 5. Configure Spark-Kafka consumer options and Subscribe to Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9c1b40-46a6-4629-834e-5df667667891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configure spark kafka client options\n",
    "spark_kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": config[\"global\"][\"kafka_bootstrap_servers\"],\n",
    "    \"subscribe\": config[\"global\"][\"kafka_topic\"],\n",
    "    \"kafka.group.id\": config[\"global\"][\"kafka_consumer_group\"],\n",
    "    \"maxOffsetsPerTrigger\": config[\"global\"][\"max_records_per_batch\"],\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "}\n",
    "\n",
    "# Enable spark read stream\n",
    "df = spark.readStream \\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", spark_kafka_options[\"kafka.bootstrap.servers\"]) \\\n",
    ".option(\"subscribe\", spark_kafka_options[\"subscribe\"]) \\\n",
    ".option(\"kafka.group.id\", spark_kafka_options[\"kafka.group.id\"]) \\\n",
    ".option(\"maxOffsetsPerTrigger\", spark_kafka_options[\"maxOffsetsPerTrigger\"]) \\\n",
    ".option(\"startingOffsets\", spark_kafka_options[\"startingOffsets\"]) \\\n",
    ".load()\n",
    "# .schema(schema) \\\n",
    "\n",
    "# newDF = streamDF.select(from_json(col(\"value\"), schema).as(\"data\"))\n",
    "#    .select(\"data.*\")\n",
    "print(df)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78be17b4-7074-4e68-9b84-0cf90627a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a997dbe-d92d-4ca4-acac-7d8fa8bfbf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- awards: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- award: string (nullable = true)\n",
      " |    |    |-- by: string (nullable = true)\n",
      " |    |    |-- year: string (nullable = true)\n",
      " |-- birth: string (nullable = true)\n",
      " |-- contribs: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- death: string (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- aka: string (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- last: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "DataFrame[awards: array<struct<award:string,by:string,year:string>>, birth: string, contribs: array<string>, death: string, name: struct<aka:string,first:string,last:string>, title: string]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column, expr\n",
    "def func_call(df, batch_id):\n",
    "    df.selectExpr(\"CAST(value AS STRING) as json\")\n",
    "    requests = df.rdd.map(lambda x: x.value).collect()\n",
    "    logging.info(requests)\n",
    "# import spark.implicits._\n",
    "# streamDF = df.selectExpr( \"CAST(key AS STRING)\",\"CAST(value AS STRING)\")\n",
    "\n",
    "streamDF = df.selectExpr(\"CAST(value AS STRING)\").select(F.from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "        \n",
    "# .select(F.from_json(\"value\", schema))\n",
    "streamDF.printSchema()\n",
    "print(streamDF)\n",
    "# new_df = df.withWatermark(\"timestamp\", \"1 seconds\") \\\n",
    "#     .groupBy(col(\"value\")).count()\n",
    "# new_df = new_df.selectExpr(\"CAST(value AS STRING)\", \"CAST(count AS STRING)\")\n",
    "# query = streamDF \\\n",
    "#         .writeStream \\\n",
    "#         .format(\"kafka\") \\\n",
    "#         .option(\"kafka.bootstrap.servers\", spark_kafka_options[\"kafka.bootstrap.servers\"]) \\\n",
    "#         .option(\"checkpointLocation\", \"/opt/bitnami/check7\") \\\n",
    "#         .option(\"topic\", \"first_topic\") \\\n",
    "#         .start()\n",
    "# query.awaitTermination()\n",
    "# query = streamDF \\\n",
    "#         .writeStream \\\n",
    "#         .format(\"console\") \\\n",
    "#         .start()\n",
    "# print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b022a19-12ed-46ab-800f-97503e2a6917",
   "metadata": {},
   "source": [
    "# 6. Start spark structred streaming job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aaa8dd8-4f4a-4fca-8d4e-7386989b8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonFileHandler(object):\n",
    "    def __init__(self, file_path:str, mode:str):\n",
    "        \"\"\"\n",
    "        Initialize file handler\n",
    "        \"\"\"\n",
    "        \n",
    "        self.f = open(file_path, mode)\n",
    "    \n",
    "    def write_dataframe_as_jsonl(self, batch_df):\n",
    "        \"\"\"\n",
    "        Write a micro-batch dataframe to a jsonl file \n",
    "        \"\"\"\n",
    "        for row in batch_df.collect():\n",
    "            res = json.dumps(row.asDict())\n",
    "            self.f.write(res+\"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Finalize file write object\n",
    "        \"\"\"\n",
    "        self.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4934c6a5-164b-42e6-9e51-963b420490e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.java_gateway:Callback Server Starting\n",
      "INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 39077)\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lambda Function for processing each batch of record\n",
    "def process_batch(batch_df, batch_idx):\n",
    "    print(f\"{batch_idx} | {batch_df.count()}\")\n",
    "    \n",
    "    json_file_writer = JsonFileHandler(file_path=\"result.jsonl\", mode=\"w\")\n",
    "    # Process data and calculate\n",
    "    # a. age\n",
    "    # b. num_contribs\n",
    "    # c. min_max_years\n",
    "    # batch_df.printSchema()\n",
    "    \n",
    "    batch_df = batch_df.withColumn(\"age\",\n",
    "                         F.months_between(col(\"death\"), col(\"birth\"))/12)\n",
    "    \n",
    "    from pyspark.sql.types import IntegerType\n",
    "    slen = F.udf(lambda s: len(s), IntegerType())\n",
    "    batch_df = batch_df.withColumn(\"num_contribs\",\n",
    "                                  slen(col(\"contribs\")))\n",
    "    batch_df = batch_df.withColumn(\"min_y\",F.array_min(F.col('awards.year')))\n",
    "    batch_df = batch_df.withColumn(\"max_y\",F.array_max(F.col(\"awards.year\")))\n",
    "    batch_df = batch_df.withColumn(\"min_max\", F.array(F.col(\"min_y\"), F.col(\"max_y\")))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Select required columns -  \"name\", \"age\", \"num_contribs\", \"min_max\"\n",
    "    \n",
    "    selectDF = batch_df.select(\"name\", \"age\", \"num_contribs\",\"min_max\") \n",
    "\n",
    "    # Save to parquet file - result.parquet \n",
    "    \n",
    "    batch_df = selectDF\n",
    "    json_file_writer.write_dataframe_as_jsonl(batch_df)\n",
    "    json_file_writer.close()\n",
    "    return batch_df\n",
    "\n",
    "# Structred streaming query\n",
    "query = streamDF \\\n",
    ".writeStream \\\n",
    ".foreachBatch(process_batch) \\\n",
    ".outputMode(\"update\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08164cc-18ff-4e14-82b2-6b69af325de0",
   "metadata": {},
   "source": [
    "# 7. Monitor structred streaming job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c34f9d7b-bd97-42db-b5c9-74373f999bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:consumer:Structred streaming job completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# Add startup delay\n",
    "time.sleep(5)\n",
    "# Update Job Status\n",
    "\n",
    "print(query.status)\n",
    "while query.status['isDataAvailable'] or query.status['isTriggerActive']:\n",
    "    print(query.status)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Stop query\n",
    "query.stop()\n",
    "\n",
    "logger.info(\"Structred streaming job completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b1b43-f9e4-4a89-ae92-0676de17277f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5eab687acb3ddfe264791fe74937bc8765d50ea3df4d9a9a62730aa97325aae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
